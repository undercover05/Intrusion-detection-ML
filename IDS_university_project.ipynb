{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021cfc04",
   "metadata": {},
   "source": [
    "# Intrusion Detection \n",
    "This notebook demonstrates **data cleaning**, **feature engineering**, **feature selection**, and **model training** on the uploaded `cybersecurity_intrusion_data.csv`. It's prepared as a university project deliverable: readable, reproducible, and annotated.\n",
    "\n",
    "It contains example code for hyperparameter tuning and for model explanation (SHAP). Run cells in order.\n",
    "\n",
    "Author: ChatGPT (ML training assistant)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fa72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import os\n",
    "print('Environment ready, pandas', pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096497c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "fn = '/mnt/data/cybersecurity_intrusion_data.csv'\n",
    "df = pd.read_csv(fn)\n",
    "print('Shape:', df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "print('Columns and dtypes:')\n",
    "display(df.dtypes)\n",
    "print('\\nMissing values per column:')\n",
    "display(df.isnull().sum().sort_values(ascending=False))\n",
    "print('\\nValue counts for object columns (sample):')\n",
    "for c in df.select_dtypes(include=['object','category']).columns[:5]:\n",
    "    print('\\n---', c, '---')\n",
    "    display(df[c].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5d67d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target column (heuristic). If the dataset has an obvious 'label' or 'attack' column it will be used,\n",
    "# otherwise the last column is used as the target.\n",
    "possible_targets = [c for c in df.columns if c.lower() in ('label','target','attack','intrusion','class','is_intrusion','attack_type')]\n",
    "if len(possible_targets) == 0:\n",
    "    target_col = df.columns[-1]\n",
    "    print('No canonical target found. Using last column as target:', target_col)\n",
    "else:\n",
    "    target_col = possible_targets[0]\n",
    "    print('Using target column:', target_col)\n",
    "\n",
    "print('\\nTarget distribution:')\n",
    "display(df[target_col].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31102f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Drop duplicates\n",
    "dups = df_clean.duplicated().sum()\n",
    "print('Duplicate rows:', dups)\n",
    "if dups > 0:\n",
    "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Impute missing values: numeric -> median, categorical -> mode\n",
    "num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df_clean.select_dtypes(include=['object','category']).columns.tolist()\n",
    "if target_col in cat_cols:\n",
    "    cat_cols.remove(target_col)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "num_imp = SimpleImputer(strategy='median')\n",
    "cat_imp = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "if len(num_cols) > 0:\n",
    "    df_clean[num_cols] = num_imp.fit_transform(df_clean[num_cols])\n",
    "if len(cat_cols) > 0:\n",
    "    df_clean[cat_cols] = cat_imp.fit_transform(df_clean[cat_cols])\n",
    "\n",
    "print('After imputation, any missing?', df_clean.isnull().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f33da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (examples tailored to common IDS features).\n",
    "# Add derived features only if the base columns exist in the dataset.\n",
    "df_fe = df_clean.copy()\n",
    "\n",
    "derived = {}\n",
    "if 'src_bytes' in df_fe.columns and 'dst_bytes' in df_fe.columns:\n",
    "    df_fe['bytes_ratio_src_dst'] = df_fe['src_bytes'] / (df_fe['dst_bytes'] + 1)\n",
    "    derived['bytes_ratio_src_dst'] = 'src_bytes/dst_bytes'\n",
    "if 'session_duration' in df_fe.columns and 'network_packet_size' in df_fe.columns:\n",
    "    df_fe['packets_per_second'] = df_fe['network_packet_size'] / (df_fe['session_duration'] + 1e-6)\n",
    "    derived['packets_per_second'] = 'network_packet_size/session_duration'\n",
    "if 'login_attempts' in df_fe.columns and 'session_duration' in df_fe.columns:\n",
    "    df_fe['attempts_per_min'] = df_fe['login_attempts'] / (df_fe['session_duration']/60 + 1e-6)\n",
    "    derived['attempts_per_min'] = 'login_attempts/(duration/60)'\n",
    "\n",
    "print('Derived features created:', derived)\n",
    "df_fe.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97597b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y and encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "y = df_fe[target_col].astype(str)\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "print('Classes:', dict(enumerate(le.classes_)))\n",
    "\n",
    "X = df_fe.drop(columns=[target_col]).copy()\n",
    "\n",
    "# Separate categorical cols\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# For categorical features:\n",
    "# - if unique values <= 20 -> OneHot\n",
    "# - else -> LabelEncode\n",
    "low_card = [c for c in cat_cols if X[c].nunique() <= 20]\n",
    "high_card = [c for c in cat_cols if X[c].nunique() > 20]\n",
    "print('Low-card categorical:', low_card)\n",
    "print('High-card categorical:', high_card)\n",
    "\n",
    "# Label-encode high-cardinality\n",
    "for c in high_card:\n",
    "    X[c] = LabelEncoder().fit_transform(X[c].astype(str))\n",
    "\n",
    "# One-hot low-cardinality\n",
    "if len(low_card) > 0:\n",
    "    X = pd.get_dummies(X, columns=low_card, drop_first=True)\n",
    "\n",
    "print('Shape after encoding:', X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a11e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection pipeline:\n",
    "# 1) VarianceThreshold to remove near-constant columns\n",
    "# 2) Drop highly correlated features (> 0.95)\n",
    "# 3) SelectKBest (mutual information) to pick top features\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, mutual_info_classif\n",
    "\n",
    "X_num = X.fillna(0).copy()\n",
    "vt = VarianceThreshold(threshold=1e-5)\n",
    "vt.fit(X_num)\n",
    "cols_vt = X_num.columns[vt.get_support()]\n",
    "X_vt = X_num[cols_vt]\n",
    "\n",
    "# Drop highly correlated\n",
    "corr = X_vt.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [c for c in upper.columns if any(upper[c] > 0.95)]\n",
    "print('Dropping due to high correlation:', to_drop)\n",
    "X_vt.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "# SelectKBest\n",
    "k = min(30, X_vt.shape[1])\n",
    "skb = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "skb.fit(X_vt, y_enc)\n",
    "mask = skb.get_support()\n",
    "selected_features = X_vt.columns[mask].tolist()\n",
    "print('Selected features ({}):'.format(len(selected_features)))\n",
    "display(pd.DataFrame({'feature': X_vt.columns, 'score': skb.scores_}).sort_values('score', ascending=False).head(40))\n",
    "\n",
    "X_sel = X_vt[selected_features].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec116d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split and scaling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sel, y_enc, test_size=0.3, random_state=42, stratify=y_enc)\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print('Train shape:', X_train_s.shape, 'Test shape:', X_test_s.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=200, random_state=42)\n",
    "}\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "results = {}\n",
    "for name, m in models.items():\n",
    "    print('\\nTraining', name)\n",
    "    m.fit(X_train_s, y_train)\n",
    "    preds = m.predict(X_test_s)\n",
    "    print('Classification report for', name)\n",
    "    print(classification_report(y_test, preds))\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "    results[name] = m\n",
    "\n",
    "# Save best model (example: GradientBoosting)\n",
    "joblib.dump(results['GradientBoosting'], 'best_model_joblib.pkl')\n",
    "print('Saved best model to best_model_joblib.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning example (RandomizedSearchCV on RandomForest)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': sp_randint(50, 400),\n",
    "    'max_depth': sp_randint(3, 20),\n",
    "    'min_samples_split': sp_randint(2, 20),\n",
    "    'min_samples_leaf': sp_randint(1, 20)\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rs = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=30, scoring='f1', cv=3, verbose=1, n_jobs=-1, random_state=42)\n",
    "rs.fit(X_train_s, y_train)\n",
    "print('Best params:', rs.best_params_)\n",
    "print('Best CV score:', rs.best_score_)\n",
    "best_rf = rs.best_estimator_\n",
    "# Evaluate on test set\n",
    "preds = best_rf.predict(X_test_s)\n",
    "print('\\nTest classification report (best RF):')\n",
    "print(classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4958b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model explainability with SHAP (if available).\n",
    "# If SHAP is not installed, uncomment the pip install line and run the cell.\n",
    "try:\n",
    "    import shap\n",
    "    print('SHAP version', shap.__version__)\n",
    "    explainer = shap.Explainer(results['RandomForest'], X_train_s)\n",
    "    shap_values = explainer(X_test_s)\n",
    "    # Plot summary (force matplotlib)\n",
    "    shap.summary_plot(shap_values, features=X_test, feature_names=X_test.columns, show=True)\n",
    "except Exception as e:\n",
    "    print('SHAP not available or failed to run:', e)\n",
    "    print('If you want SHAP, run: pip install shap')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset and selected features to /mnt/data for submission\n",
    "X_sel.to_csv('/mnt/data/selected_features.csv', index=False)\n",
    "df_fe.head(200).to_csv('/mnt/data/cleaned_dataset_sample.csv', index=False)\n",
    "print('Saved selected_features.csv and cleaned_dataset_sample.csv in /mnt/data') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c61df9",
   "metadata": {},
   "source": [
    "# Final notes\n",
    "\n",
    "\"Suggestions for the report\"\n",
    "\n",
    "\"1. Describe data provenance and any assumptions (e.g., how target was defined).\"\n",
    "\"2. Show EDA plots (class imbalance, feature distributions).\" \n",
    "\"3. Report cross-validated metrics and confusion matrix with per-class precision/recall.\" \n",
    "\"4. Discuss limitations and next steps.\"\n",
    "\n",
    "Good luck with your project â€” you can run each cell in order and modify hyperparameters, add visualizations, or extend with other models (XGBoost/LightGBM) if desired.\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
